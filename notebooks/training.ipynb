{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FnRd62OY8ru",
        "outputId": "e019bbda-eb23-41cc-e577-fdf1b535b71b"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 9.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 39.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 39.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 36.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n",
            "Collecting datasets\n",
            "  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
            "\u001b[K     |████████████████████████████████| 264 kB 8.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.0)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.12)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.4)\n",
            "Collecting fsspec>=2021.05.0\n",
            "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 42.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 51.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, fsspec, datasets\n",
            "Successfully installed datasets-1.11.0 fsspec-2021.7.0 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4Jn5P1CXzkM",
        "outputId": "6713c579-4d8b-48d1-ff56-f61ac79f9a34"
      },
      "source": [
        "!git clone https://github.com/hopl1t/frankenbert.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'frankenbert'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 24 (delta 5), reused 23 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (24/24), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQc-Vd-nYk1_"
      },
      "source": [
        "import os\n",
        "os.chdir('frankenbert')\n",
        "from trainer import *\n",
        "import utils"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35asmgJtYqCl"
      },
      "source": [
        "!python trainer.py -m distilgpt2 -t causal -d wikitext,wikitext-2-raw-v1 -s ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPCM1-nMfwwO"
      },
      "source": [
        "model = torch.load('./distilgpt2_wikitext_2108281508.pkl')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JURJmR3dhQwv"
      },
      "source": [
        "# this can now also be loaded from a pickle\n",
        "model_name = 'distilgpt2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "x5k0_u-TgG5T",
        "outputId": "be56ac97-3641-4443-e707-2f2bbce0169c"
      },
      "source": [
        "utils.generate(model, 'Hello I am very', tokenizer)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello I am very sorry for the inconvenience I had caused to you. \" He said to Princess Charlotte, which he \"s probably a very small part of. \" After his victory, his wife replied to him : \" I should just take it as an insult to her. You are too weak a monster to be considered in this world, but I wish I would only let you live in poverty without you. \" \\n He also praised their relationship, saying, \" As a father you were always a very good husband, and I know that if you die, you would always be the best husband. You have always said, Oh my God I am.\\'\" \\n She was then taken into custody and put in psychiatric ward, before receiving any legal advice from a juvenile court. Prince Philip and Duchess Charlotte began visiting with his younger sister. He invited Charlotte to an upcoming wedding in London, where he had been looking for a daughter, Kate. His sister asked him to marry Kate, but the two agreed that they could not stay for long. Charlotte and Prince Philip agreed to spend his time at his father\\'s house together, and it was his best wishes. \" \\n = = = The Prince\\'s Tale = = = '"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "FFw8r_ClkB5K",
        "outputId": "aac8a703-0059-4c30-d085-19dfac353a36"
      },
      "source": [
        "utils.predict_next(model, 'Hello I am very', tokenizer)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-6df88fe1de26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Hello I am very'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/frankenbert/utils.py\u001b[0m in \u001b[0;36mpredict_next\u001b[0;34m(model, sequence, tokenizer)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_next_token_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mnext_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mresulting_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpuJ2c5TnCQZ",
        "outputId": "64a29e7a-511b-495c-84ae-0ecd7d387d94"
      },
      "source": [
        "!git checkout nir"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Branch 'nir' set up to track remote branch 'nir' from 'origin'.\n",
            "Switched to a new branch 'nir'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iep6SdUZh5cY",
        "outputId": "80c21290-b866-4051-9009-f2f7b13e755d"
      },
      "source": [
        "!python trainer.py -m distilroberta-base -t MLM -d wikitext,wikitext-2-raw-v1 -s ./ -e 1"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rDownloading:   0% 0.00/1.93k [00:00<?, ?B/s]\rDownloading: 8.33kB [00:00, 10.5MB/s]       \n",
            "\rDownloading:   0% 0.00/1.12k [00:00<?, ?B/s]\rDownloading: 5.83kB [00:00, 7.45MB/s]       \n",
            "Downloading and preparing dataset wikitext/wikitext-2-raw-v1 (download: 4.50 MiB, generated: 12.91 MiB, post-processed: Unknown size, total: 17.41 MiB) to /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20...\n",
            "Downloading: 100% 4.72M/4.72M [00:00<00:00, 7.95MB/s]\n",
            "Dataset wikitext downloaded and prepared to /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20. Subsequent calls will reuse this data.\n",
            "Downloading: 100% 480/480 [00:00<00:00, 763kB/s]\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 2.45MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 1.55MB/s]\n",
            "Downloading: 100% 1.36M/1.36M [00:00<00:00, 3.02MB/s]\n",
            " #0:   0% 0/3 [00:00<?, ?ba/s]\n",
            " #0:  33% 1/3 [00:00<00:00,  2.96ba/s]\n",
            " #0:  67% 2/3 [00:00<00:00,  3.19ba/s]Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n",
            "\n",
            " #0: 100% 3/3 [00:00<00:00,  4.53ba/s]\n",
            " #1: 100% 3/3 [00:00<00:00,  4.58ba/s]\n",
            " #0:   0% 0/19 [00:00<?, ?ba/s]\n",
            " #1:   0% 0/19 [00:00<?, ?ba/s]\u001b[AToken indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n",
            "\n",
            " #0:   5% 1/19 [00:00<00:05,  3.19ba/s]\n",
            " #1:  11% 2/19 [00:00<00:05,  3.30ba/s]\u001b[AToken indices sequence length is longer than the specified maximum sequence length for this model (638 > 512). Running this sequence through the model will result in indexing errors\n",
            " #0:  16% 3/19 [00:00<00:04,  3.26ba/s]\n",
            " #1:  16% 3/19 [00:00<00:04,  3.25ba/s]\u001b[A\n",
            " #0:  26% 5/19 [00:01<00:03,  3.55ba/s]\n",
            " #0:  32% 6/19 [00:01<00:03,  3.60ba/s]\n",
            " #1:  32% 6/19 [00:01<00:03,  3.60ba/s]\u001b[A\n",
            " #0:  37% 7/19 [00:02<00:03,  3.26ba/s]\n",
            " #0:  42% 8/19 [00:02<00:03,  3.25ba/s]\n",
            " #0:  47% 9/19 [00:02<00:03,  3.27ba/s]\n",
            " #0:  53% 10/19 [00:03<00:02,  3.27ba/s]\n",
            " #0:  58% 11/19 [00:03<00:02,  3.24ba/s]\n",
            " #0:  63% 12/19 [00:03<00:02,  3.28ba/s]\n",
            " #0:  74% 14/19 [00:04<00:01,  3.28ba/s]\n",
            " #0:  79% 15/19 [00:04<00:01,  3.25ba/s]\n",
            " #0:  84% 16/19 [00:04<00:00,  3.06ba/s]\n",
            " #0:  89% 17/19 [00:05<00:00,  3.09ba/s]\n",
            " #0:  95% 18/19 [00:05<00:00,  3.15ba/s]\n",
            " #0: 100% 19/19 [00:05<00:00,  3.35ba/s]\n",
            "\n",
            " #1: 100% 19/19 [00:05<00:00,  3.35ba/s]\n",
            " #0:   0% 0/2 [00:00<?, ?ba/s]\n",
            " #0:  50% 1/2 [00:00<00:00,  3.45ba/s]\n",
            " #0: 100% 2/2 [00:00<00:00,  3.40ba/s]\n",
            "\n",
            " #1: 100% 2/2 [00:00<00:00,  3.32ba/s]\n",
            " #0:   0% 0/3 [00:00<?, ?ba/s]\n",
            " #0:  33% 1/3 [00:00<00:01,  1.43ba/s]\n",
            " #0:  67% 2/3 [00:01<00:00,  1.57ba/s]\n",
            " #0: 100% 3/3 [00:01<00:00,  2.29ba/s]\n",
            " #1: 100% 3/3 [00:01<00:00,  2.26ba/s]\n",
            " #0:   0% 0/19 [00:00<?, ?ba/s]\n",
            " #1:   0% 0/19 [00:00<?, ?ba/s]\u001b[A\n",
            " #0:   5% 1/19 [00:00<00:10,  1.73ba/s]\n",
            " #0:  11% 2/19 [00:01<00:10,  1.56ba/s]\n",
            " #0:  16% 3/19 [00:01<00:09,  1.61ba/s]\n",
            " #0:  26% 5/19 [00:02<00:08,  1.75ba/s]\n",
            " #1:  26% 5/19 [00:02<00:08,  1.68ba/s]\u001b[A\n",
            " #0:  32% 6/19 [00:03<00:07,  1.76ba/s]\n",
            " #0:  37% 7/19 [00:04<00:07,  1.65ba/s]\n",
            " #0:  42% 8/19 [00:04<00:06,  1.70ba/s]\n",
            " #0:  47% 9/19 [00:05<00:05,  1.71ba/s]\n",
            " #0:  53% 10/19 [00:05<00:05,  1.68ba/s]\n",
            " #0:  58% 11/19 [00:06<00:04,  1.67ba/s]\n",
            " #0:  68% 13/19 [00:07<00:03,  1.63ba/s]\n",
            " #0:  74% 14/19 [00:08<00:02,  1.69ba/s]\n",
            " #0:  79% 15/19 [00:08<00:02,  1.71ba/s]\n",
            " #0:  84% 16/19 [00:09<00:01,  1.64ba/s]\n",
            " #0:  89% 17/19 [00:10<00:01,  1.62ba/s]\n",
            " #0: 100% 19/19 [00:10<00:00,  1.74ba/s]\n",
            "\n",
            " #1: 100% 19/19 [00:11<00:00,  1.73ba/s]\n",
            " #0:   0% 0/2 [00:00<?, ?ba/s]\n",
            " #0:  50% 1/2 [00:00<00:00,  1.78ba/s]\n",
            " #0: 100% 2/2 [00:01<00:00,  1.86ba/s]\n",
            "\n",
            " #1: 100% 2/2 [00:01<00:00,  1.76ba/s]\n",
            "Downloading: 100% 331M/331M [00:08<00:00, 40.8MB/s]\n",
            "***** Running training *****\n",
            "  Num examples = 19242\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2406\n",
            "{'loss': 2.284, 'learning_rate': 1.5843724023275145e-05, 'epoch': 0.21}\n",
            " 21% 500/2406 [00:53<03:21,  9.44it/s]Saving model checkpoint to test-clm/checkpoint-500\n",
            "Configuration saved in test-clm/checkpoint-500/config.json\n",
            "Model weights saved in test-clm/checkpoint-500/pytorch_model.bin\n",
            "{'loss': 2.1464, 'learning_rate': 1.1687448046550292e-05, 'epoch': 0.42}\n",
            " 42% 1000/2406 [01:50<02:30,  9.37it/s]Saving model checkpoint to test-clm/checkpoint-1000\n",
            "Configuration saved in test-clm/checkpoint-1000/config.json\n",
            "Model weights saved in test-clm/checkpoint-1000/pytorch_model.bin\n",
            "{'loss': 2.0922, 'learning_rate': 7.531172069825436e-06, 'epoch': 0.62}\n",
            " 62% 1500/2406 [02:46<01:35,  9.46it/s]Saving model checkpoint to test-clm/checkpoint-1500\n",
            "Configuration saved in test-clm/checkpoint-1500/config.json\n",
            "Model weights saved in test-clm/checkpoint-1500/pytorch_model.bin\n",
            "{'loss': 2.0666, 'learning_rate': 3.374896093100582e-06, 'epoch': 0.83}\n",
            " 83% 2000/2406 [03:43<00:42,  9.47it/s]Saving model checkpoint to test-clm/checkpoint-2000\n",
            "Configuration saved in test-clm/checkpoint-2000/config.json\n",
            "Model weights saved in test-clm/checkpoint-2000/pytorch_model.bin\n",
            "100% 2405/2406 [04:30<00:00,  9.37it/s]***** Running Evaluation *****\n",
            "  Num examples = 1988\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/249 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 5/249 [00:00<00:05, 42.49it/s]\u001b[A\n",
            "  4% 10/249 [00:00<00:06, 36.52it/s]\u001b[A\n",
            "  6% 14/249 [00:00<00:06, 35.11it/s]\u001b[A\n",
            "  7% 18/249 [00:00<00:06, 34.31it/s]\u001b[A\n",
            "  9% 22/249 [00:00<00:06, 33.95it/s]\u001b[A\n",
            " 10% 26/249 [00:00<00:06, 33.67it/s]\u001b[A\n",
            " 12% 30/249 [00:00<00:06, 33.51it/s]\u001b[A\n",
            " 14% 34/249 [00:00<00:06, 33.46it/s]\u001b[A\n",
            " 15% 38/249 [00:01<00:06, 33.10it/s]\u001b[A\n",
            " 17% 42/249 [00:01<00:06, 33.33it/s]\u001b[A\n",
            " 18% 46/249 [00:01<00:06, 33.31it/s]\u001b[A\n",
            " 20% 50/249 [00:01<00:05, 33.28it/s]\u001b[A\n",
            " 22% 54/249 [00:01<00:05, 33.25it/s]\u001b[A\n",
            " 23% 58/249 [00:01<00:05, 32.92it/s]\u001b[A\n",
            " 25% 62/249 [00:01<00:05, 33.29it/s]\u001b[A\n",
            " 27% 66/249 [00:01<00:05, 33.23it/s]\u001b[A\n",
            " 28% 70/249 [00:02<00:05, 33.14it/s]\u001b[A\n",
            " 30% 74/249 [00:02<00:05, 33.16it/s]\u001b[A\n",
            " 31% 78/249 [00:02<00:05, 33.17it/s]\u001b[A\n",
            " 33% 82/249 [00:02<00:05, 33.15it/s]\u001b[A\n",
            " 35% 86/249 [00:02<00:04, 33.05it/s]\u001b[A\n",
            " 36% 90/249 [00:02<00:04, 33.13it/s]\u001b[A\n",
            " 38% 94/249 [00:02<00:04, 33.14it/s]\u001b[A\n",
            " 39% 98/249 [00:02<00:04, 32.85it/s]\u001b[A\n",
            " 41% 102/249 [00:03<00:04, 33.23it/s]\u001b[A\n",
            " 43% 106/249 [00:03<00:04, 33.21it/s]\u001b[A\n",
            " 44% 110/249 [00:03<00:04, 33.12it/s]\u001b[A\n",
            " 46% 114/249 [00:03<00:04, 33.14it/s]\u001b[A\n",
            " 47% 118/249 [00:03<00:03, 33.00it/s]\u001b[A\n",
            " 49% 122/249 [00:03<00:03, 33.18it/s]\u001b[A\n",
            " 51% 126/249 [00:03<00:03, 33.17it/s]\u001b[A\n",
            " 52% 130/249 [00:03<00:03, 33.12it/s]\u001b[A\n",
            " 54% 134/249 [00:04<00:03, 33.17it/s]\u001b[A\n",
            " 55% 138/249 [00:04<00:03, 32.97it/s]\u001b[A\n",
            " 57% 142/249 [00:04<00:03, 33.09it/s]\u001b[A\n",
            " 59% 146/249 [00:04<00:03, 33.09it/s]\u001b[A\n",
            " 60% 150/249 [00:04<00:02, 33.10it/s]\u001b[A\n",
            " 62% 154/249 [00:04<00:02, 33.11it/s]\u001b[A\n",
            " 63% 158/249 [00:04<00:02, 33.17it/s]\u001b[A\n",
            " 65% 162/249 [00:04<00:02, 33.19it/s]\u001b[A\n",
            " 67% 166/249 [00:04<00:02, 33.18it/s]\u001b[A\n",
            " 68% 170/249 [00:05<00:02, 33.16it/s]\u001b[A\n",
            " 70% 174/249 [00:05<00:02, 33.14it/s]\u001b[A\n",
            " 71% 178/249 [00:05<00:02, 32.94it/s]\u001b[A\n",
            " 73% 182/249 [00:05<00:02, 33.15it/s]\u001b[A\n",
            " 75% 186/249 [00:05<00:01, 33.17it/s]\u001b[A\n",
            " 76% 190/249 [00:05<00:01, 33.20it/s]\u001b[A\n",
            " 78% 194/249 [00:05<00:01, 33.19it/s]\u001b[A\n",
            " 80% 198/249 [00:05<00:01, 33.20it/s]\u001b[A\n",
            " 81% 202/249 [00:06<00:01, 33.23it/s]\u001b[A\n",
            " 83% 206/249 [00:06<00:01, 33.17it/s]\u001b[A\n",
            " 84% 210/249 [00:06<00:01, 33.20it/s]\u001b[A\n",
            " 86% 214/249 [00:06<00:01, 33.24it/s]\u001b[A\n",
            " 88% 218/249 [00:06<00:00, 33.13it/s]\u001b[A\n",
            " 89% 222/249 [00:06<00:00, 33.18it/s]\u001b[A\n",
            " 91% 226/249 [00:06<00:00, 33.01it/s]\u001b[A\n",
            " 92% 230/249 [00:06<00:00, 33.15it/s]\u001b[A\n",
            " 94% 234/249 [00:07<00:00, 33.21it/s]\u001b[A\n",
            " 96% 238/249 [00:07<00:00, 33.00it/s]\u001b[A\n",
            " 97% 242/249 [00:07<00:00, 33.14it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.9363304376602173, 'eval_runtime': 7.5043, 'eval_samples_per_second': 264.913, 'eval_steps_per_second': 33.181, 'epoch': 1.0}\n",
            "100% 2406/2406 [04:37<00:00,  9.37it/s]\n",
            "100% 249/249 [00:07<00:00, 33.02it/s]\u001b[A\n",
            "                                     \u001b[A\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 277.8981, 'train_samples_per_second': 69.241, 'train_steps_per_second': 8.658, 'train_loss': 2.1322051672170486, 'epoch': 1.0}\n",
            "100% 2406/2406 [04:37<00:00,  8.66it/s]\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1988\n",
            "  Batch size = 8\n",
            "100% 249/249 [00:07<00:00, 33.25it/s]\n",
            "Perplexity: 6.89\n",
            "Saved model to ./distilroberta-base_wikitext_2108281608.pkl\n",
            "Saved tokenizer to ./distilroberta-base_tokenizer_2108281608.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOdN-2mcqaf8"
      },
      "source": [
        "model = torch.load('./distilroberta-base_wikitext_2108281608.pkl')\n",
        "with open('./distilroberta-base_tokenizer_2108281608.pkl', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8URyvLKqj1b",
        "outputId": "c225aa0c-ebe3-43e0-87b7-815fa6d16810"
      },
      "source": [
        "predictions = utils.mlm(model, f'Not since {tokenizer.mask_token} did the english went to war', tokenizer)\n",
        "for p in predictions:\n",
        "    print(p)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.158:\tNot since == 1914 == did the english went to war\n",
            "0.0808:\tNot since == 1917 == did the english went to war\n",
            "0.0657:\tNot since == 1918 == did the english went to war\n",
            "0.0405:\tNot since == 1945 == did the english went to war\n",
            "0.0397:\tNot since == war == did the english went to war\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}